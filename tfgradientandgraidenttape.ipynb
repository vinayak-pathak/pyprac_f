{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfgradientandgraidenttape.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayak-pathak/pyprac_f/blob/master/tfgradientandgraidenttape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRTWbbfiulwC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "059d095e-c361-48c0-fb5a-ad20d0a9157d"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras as tfk\n",
        "\n",
        "\n",
        "class Test(tfk.Model):\n",
        "    def __init__(self):\n",
        "        super(Test, self).__init__()\n",
        "        self.embedding_layer = tfk.layers.Embedding(50000, 300)\n",
        "        self.conv1d_layer = tfk.layers.Conv1D(256, 5)\n",
        "        self.pool_layer = tfk.layers.MaxPool1D(pool_size=5, strides=2)\n",
        "        self.dense1_layer = tfk.layers.Dense(128, activation=tfk.activations.swish)\n",
        "        self.dense2_layer = tfk.layers.Dense(10, activation=tfk.activations.softmax)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        hidden = self.embedding_layer(inputs)\n",
        "        hidden = self.conv1d_layer(hidden)\n",
        "        hidden = self.pool_layer(hidden)\n",
        "        hidden = tfk.layers.Flatten()(hidden)\n",
        "        hidden = self.dense1_layer(hidden)\n",
        "        y_pred = self.dense2_layer(hidden)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "class Test2(tfk.Model):\n",
        "    def __init__(self):\n",
        "        super(Test2, self).__init__()\n",
        "        self.embedding_layer = tfk.layers.Embedding(50000, 300)\n",
        "        self.rnn_layer = tfk.layers.LSTM(200)\n",
        "        self.dense1_layer = tfk.layers.Dense(128, activation=tfk.activations.swish)\n",
        "        self.dense2_layer = tfk.layers.Dense(10, activation=tfk.activations.softmax)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        hidden = self.embedding_layer(inputs)\n",
        "        hidden = self.rnn_layer(hidden)\n",
        "        hidden = self.dense1_layer(hidden)\n",
        "        y_pred = self.dense2_layer(hidden)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "epochs = 5\n",
        "x = np.random.randint(low=0, high=50000, size=(10000, 128))\n",
        "y = np.random.randint(low=0, high=10, size=(10000,))\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n",
        "trainset = tf.data.Dataset.zip(\n",
        "    (tf.data.Dataset.from_tensor_slices(x_train), tf.data.Dataset.from_tensor_slices(y_train))).batch(300)\n",
        "valset = tf.data.Dataset.zip(\n",
        "    (tf.data.Dataset.from_tensor_slices(x_val), tf.data.Dataset.from_tensor_slices(y_val))).batch(300)\n",
        "# model = Test()\n",
        "model = Test2()\n",
        "train_acc = tf.metrics.SparseCategoricalAccuracy()\n",
        "val_acc = tf.metrics.SparseCategoricalAccuracy()\n",
        "train_loss = tf.metrics.Mean()\n",
        "val_loss = tf.metrics.Mean()\n",
        "loss_object = tf.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.optimizers.Adam()\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_object, metrics=[train_acc])\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=300, epochs=epochs)\n",
        "\n",
        "# model = Test()\n",
        "model = Test2()\n",
        "optimizer = tfk.mixed_precision.experimental.LossScaleOptimizer(optimizer, \"dynamic\")\n",
        "\n",
        "@tf.function\n",
        "def train_op(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(x)\n",
        "        loss = loss_object(y, y_pred)\n",
        "        scaled_loss = optimizer.get_scaled_loss(loss)\n",
        "    scaled_grads = tape.gradient(scaled_loss, model.trainable_weights)\n",
        "    grads = optimizer.get_unscaled_gradients(scaled_grads)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_loss.update_state(loss)\n",
        "    train_acc.update_state(y, y_pred)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def val_op(x, y):\n",
        "    y_pred = model(x)\n",
        "    loss = loss_object(y, y_pred)\n",
        "    val_loss.update_state(loss)\n",
        "    val_acc.update_state(y, y_pred)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Epoch {}/{}\".format(epoch + 1, epochs))\n",
        "    # remember reset states hereï¼Œthen GradientTape will get similar loss & acc with model.fit\n",
        "    train_loss.reset_states()\n",
        "    train_acc.reset_states()\n",
        "    val_loss.reset_states()\n",
        "    val_acc.reset_states()\n",
        "    bar = tfk.utils.Progbar(len(y_train), unit_name=\"sample\", stateful_metrics={\"loss\", \"acc\"})\n",
        "    log_values = []\n",
        "    for batch_x, batch_y in trainset:\n",
        "        train_op(batch_x, batch_y)\n",
        "        log_values.append((\"loss\", train_loss.result().numpy()))\n",
        "        log_values.append((\"acc\", train_acc.result().numpy()))\n",
        "        bar.add(len(batch_y), log_values)\n",
        "    for batch_x, batch_y in valset:\n",
        "        val_op(batch_x, batch_y)\n",
        "    template = \"val_loss: {:.4f} - val_acc: {:.4f}\"\n",
        "    print(template.format(val_loss.result().numpy(), val_acc.result().numpy()))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "16/27 [================>.............] - ETA: 22s - loss: 2.3029 - sparse_categorical_accuracy: 0.1088"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak6FF_L-eJ0K",
        "colab_type": "text"
      },
      "source": [
        "## Alternative\n",
        "model.fit vs GradientTape\n",
        "This is really a case-specific thing and it's difficult to give a definite answer here (it might border on \"too opinion-based). But in general, I would say\n",
        "\n",
        "The \"classic\" Keras interface (using compile, fitetc.) allows for quick and easy building, training & evaluation of standard models. However, it is very high-level/abstract and as such doesn't give you much low-level control. If you are implementing models with non-trivial control flow, this can be hard to accommodate.\n",
        "GradientTape gives you full low-level control over all aspects of training/running your model, allowing easier debugging as well as more complex architectures etc., but you will need to write more boilerplate code for many things that a compiled model will hide from you (e.g. training loops). Still, if you do research in deep learning you will probably be working on this level most of the time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acTbG-0OeI6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_op(x, y):\n",
        "    y_pred = model(x, training=True)\n",
        "    tf.print(y_pred) # In my case the shapes of the output required an extra tf.squeeze\n",
        "    loss = loss_object(y, y_pred)\n",
        "    train_loss.update_state(loss)\n",
        "    train_acc.update_state(y, y_pred)\n",
        "    grads = tf.gradients(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(grads, model.trainable_variables)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}